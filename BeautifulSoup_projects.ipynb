{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BeautifulSoup-projects.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC9MuuZPsjlj",
        "outputId": "11ae1969-905f-454c-85b6-5e8d9def53cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pip install bs4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YYbvW5Pst8l",
        "outputId": "82f84d5b-bb06-42a4-a696-e55cd65f41fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install requests"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPMps1C-sxJb",
        "outputId": "d6f9d1d2-d138-43bd-84d6-c0335a12501e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install texttable"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texttable\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Installing collected packages: texttable\n",
            "Successfully installed texttable-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AObhDFhfx9_d"
      },
      "source": [
        "# **P1 - Scraping Quotes from website**\n",
        "\n",
        "**URL:** [Quotes Website](http://www.values.com/inspirational-quotes)\n",
        "\n",
        "**scrapes the website and saves quotes to a file inspirational_quotes.csv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaJXCFZQx9Wb"
      },
      "source": [
        "#Python program to scrape website and save quotes to a file inspirational_quotes.csv\n",
        "\n",
        "import requests \n",
        "from bs4 import BeautifulSoup \n",
        "import csv \n",
        "\n",
        "URL = \"http://www.values.com/inspirational-quotes\"\n",
        "r = requests.get(URL) \n",
        "\n",
        "soup = BeautifulSoup(r.content, 'html5lib') \n",
        "\n",
        "quotes=[] # a list to store quotes \n",
        "\n",
        "table = soup.find('div', attrs = {'id':'all_quotes'}) \n",
        "\n",
        "for row in table.findAll('div', \n",
        "\t\t\t\t\t\tattrs = {'class':'col-6 col-lg-3 text-center margin-30px-bottom sm-margin-30px-top'}): \n",
        "\tquote = {} \n",
        "\tquote['theme'] = row.h5.text \n",
        "\tquote['url'] = row.a['href'] \n",
        "\tquote['img'] = row.img['src'] \n",
        "\tquote['lines'] = row.img['alt'].split(\" #\")[0] \n",
        "\tquote['author'] = row.img['alt'].split(\" #\")[1] \n",
        "\tquotes.append(quote) \n",
        "\n",
        "filename = 'inspirational_quotes.csv'\n",
        "with open(filename, 'w', newline='') as f: \n",
        "\tw = csv.DictWriter(f,['theme','url','img','lines','author']) \n",
        "\tw.writeheader() \n",
        "\tfor quote in quotes: \n",
        "\t\tw.writerow(quote) \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgcGY9z1tDnc"
      },
      "source": [
        "# **P2 - Scraping Covid-19 stats**\n",
        "\n",
        "URL: [COVID-19 STATS COUNTRY WISE](https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqml4Y8StUy-"
      },
      "source": [
        "# URl to Scrap: https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/\n",
        "\n",
        "import requests \n",
        "from bs4 import BeautifulSoup \n",
        "import texttable as tt\n",
        "\n",
        "# URL for scrapping data \n",
        "url = 'https://www.worldometers.info/coronavirus/countries-where-coronavirus-has-spread/'\n",
        "\n",
        "# get URL's html \n",
        "page = requests.get(url) \n",
        "soup = BeautifulSoup(page.text, 'html.parser') \n",
        "\n",
        "data = [] \n",
        "\n",
        "# soup.find_all('td') will scrape every element in the url's table \n",
        "data_iterator = iter(soup.find_all('td')) \n",
        "# data_iterator is the iterator of the table \n",
        "\n",
        "# This loop will keep repeating till there is data available in the iterator \n",
        "while True: \n",
        "\ttry: \n",
        "\t\tcountry = next(data_iterator).text \n",
        "\t\tconfirmed = next(data_iterator).text \n",
        "\t\tdeaths = next(data_iterator).text \n",
        "\t\tcontinent = next(data_iterator).text \n",
        "\n",
        "\t\t# For 'confirmed' and 'deaths', make sure to remove the commas and convert to int \n",
        "\t\tdata.append(( \n",
        "\t\t\tcountry, \n",
        "\t\t\tint(confirmed.replace(',', '')), \n",
        "\t\t\tint(deaths.replace(',', '')), \n",
        "\t\t\tcontinent \n",
        "\t\t)) \n",
        "\n",
        "\t# StopIteration error is raised when there are no more elements left to iterate through \n",
        "\texcept StopIteration: \n",
        "\t\tbreak\n",
        "\n",
        "# Sort the data by the number of confirmed cases \n",
        "data.sort(key = lambda row: row[1], reverse = True) \n",
        "\n",
        "\n",
        "# create texttable object \n",
        "table = tt.Texttable() \n",
        "table.add_rows([(None, None, None, None)] + data)  # Add an empty row at the beginning for the headers \n",
        "table.set_cols_align(('c', 'c', 'c', 'c'))  # 'l' denotes left, 'c' denotes center, and 'r' denotes right \n",
        "table.header((' Country ', ' Number of cases ', ' Deaths ', ' Continent ')) \n",
        "  \n",
        "print(table.draw())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rYb2vYHAakC"
      },
      "source": [
        "# **P3 - Scraping GPU Card Product Information**\n",
        "\n",
        "URL: [GPU Card Info](https://www.newegg.com/p/pl?d=graphics+card&nm_mc=KNC-GoogleKWLess-Search-Broad&cm_mmc=KNC-GoogleKWLess-Search-Broad-_-VGA-_-graphics-card-_-PLP-Feature&page=2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqHYj0FzAhHg"
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import urlopen as ureq\n",
        "\n",
        "my_url = 'https://www.newegg.com/p/pl?d=graphics+card&nm_mc=KNC-GoogleKWLess-Search-Broad&cm_mmc=KNC-GoogleKWLess-Search-Broad-_-VGA-_-graphics-card-_-PLP-Feature&page=2'\n",
        "uclient = ureq(my_url)\n",
        "page_html = uclient.read()\n",
        "uclient.close()\n",
        "\n",
        "page_soup = soup(page_html, \"html.parser\")\n",
        "print(page_soup)\n",
        "#print(page_soup.body.id)\n",
        "containers = page_soup.findAll(\"div\",{\"class\":\"item-container\"})\n",
        "\n",
        "filename = \"products.csv\"\n",
        "f = open(filename, \"w\")\n",
        "headers = \"brand, productname, shipping\\n\"\n",
        "f.write(headers)\n",
        "print(\"before for\")\n",
        "for container in containers:\n",
        "  print(\"after for\")\n",
        "  brand = container.div.div.a.img[\"title\"]\n",
        "  title_container = container.findAll(\"a\",{\"class\":\"item-title\"})\n",
        "  product_name = title_container[0].text\n",
        "  shipping = container.findAll(\"li\",{\"class\":\"price-ship\"})\n",
        "  shipping_price = shipping[0].text.strip()\n",
        "  print(brand)\n",
        "  print(product_name)\n",
        "  print(shipping_price)\n",
        "  f.write(brand + \",\" + product_name.replace(\",\",\"|\") + \",\" + shipping_price + \"\\n\")\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSxLggD3sDJX",
        "outputId": "bbbf61c9-b179-4580-99ba-f649c7583993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install fake_useragent"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=5a35b0342f7dca6a39a39a1558ad0d59bd3e813adf59d1e7aff8832f2547586b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aV28PoivzIm"
      },
      "source": [
        "# **P4 - Web Scraping Customer Reports**\n",
        "\n",
        "URL: http://www.consumerreports.org/cro/a-to-z-index/products/index.htm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POcOeVs9JLl_"
      },
      "source": [
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "url = 'http://www.consumerreports.org/cro/a-to-z-index/products/index.htm'                    # input your url here\n",
        "file_name = 'consumer_reports.txt'              # output file name having complete HTML content\n",
        "\n",
        "user_agent = UserAgent()\n",
        "\n",
        "page = requests.get(url,headers={'user-agent':user_agent.chrome})\n",
        "with open(file_name,'w') as file:\n",
        "    file.write(page.content.decode('utf-8')) if type(page.content) == bytes else file.write(page.content)\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def read_file():\n",
        "    file = open('consumer_reports.txt')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "\n",
        "'''\n",
        "When you inspect the consumer_report website we can see that all catagories From A - Z:\n",
        "for example air conditioners are present in 'a' tag which inside the 'div' tag.\n",
        "hence we use below code to extract all the 'a' tags.\n",
        "'''\n",
        "\n",
        "all_divs = soup.find_all('div',attrs={'class':'crux-body-copy'})\n",
        "\n",
        "#for div in all_divs:\n",
        "  #print(div.a.string)\n",
        "\n",
        "products = [div.a.string for div in all_divs]\n",
        "\n",
        "for product in products:\n",
        "    print(product)\n",
        "    print()\n",
        "\n",
        "'''\n",
        "Assignment: remove the space before ad after the string and copy the content to\n",
        "a csv file consumer_list_formatted.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kuJZLThRnsI"
      },
      "source": [
        "'''\n",
        "Here we are using the same consumer_reports.txt and creating a dictionary with\n",
        "product name as key and product link as value and trying to dispaly.\n",
        "'''\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def read_file():\n",
        "    file = open('consumer_reports.txt')\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "soup = BeautifulSoup(read_file(),'lxml')\n",
        "products = {}   # product name - key and product link - value\n",
        "\n",
        "\n",
        "product_names = [div.a.string for div in soup.find_all('div',class_='crux-body-copy')]\n",
        "\n",
        "product_links = [div.a['href'] for div in soup.find_all('div',class_='crux-body-copy')]\n",
        "\n",
        "products = {div.a.string:div.a['href'] for div in soup.find_all('div',class_='crux-body-copy')}  # Dictionary Comprahension\n",
        "\n",
        "for key,value in products.items():\n",
        "    print(key , '   -->',value)\n",
        "\n",
        "\n",
        "'''\n",
        "Assignment: remove the space before ad after the string and copy the content (Name + URL) to\n",
        "a csv file consumer_list_link.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWmFNhcwWQXr"
      },
      "source": [
        "# **P5 - Scraping Multiple web Pages**\n",
        "\n",
        "Task is to Scrap java questions from codingbat website\n",
        "\n",
        "URL: http://codingbat.com/java\n",
        "\n",
        "I will divide the project into 3 parts:\n",
        "1.   First script will describe you how to fetch the link of each section of Java questions.\n",
        "2.   Secondly we will open each section(catagory)and we scrap link for each question.\n",
        "3.   Thirdly we will open each question and get the problem statement, example associated with it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YHdi1ZOWcD_"
      },
      "source": [
        "#Part 1 - script will describe you how to fetch the link of each section of Java questions.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "user_agent = UserAgent()\n",
        "main_url = 'http://codingbat.com/java'\n",
        "page = requests.get(main_url,headers={'user-agent':user_agent.chrome})\n",
        "soup = BeautifulSoup(page.content,'lxml')\n",
        "\n",
        "base_url = 'http://codingbat.com'\n",
        "\n",
        "'''\n",
        "Here we are scraping the link to each section.\n",
        "Observe in inspect element that link is a ralative link (Warm-up) not absolute link\n",
        "thus we used base_url above\n",
        "'''\n",
        "all_divs = soup.find_all('div',class_='summ')\n",
        "\n",
        "#prints all the relative link\n",
        "for div in all_divs:\n",
        "    print(div.a['href']) #Here 'a' is a child of 'div' tag\n",
        "\n",
        "\n",
        "#prints all the absolute link\n",
        "for div in all_divs:\n",
        "    print(base_url + div.a['href'])  #Here 'a' is a child of 'div' tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwBbuCeLaOeh",
        "outputId": "6cad6add-44cc-44fb-e60f-a75ea4b02e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Secondly we will open each section and we scrap link for each question.\n",
        "#--------Start - Same as above Script ----------------------------\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "user_agent = UserAgent()\n",
        "main_url = 'http://codingbat.com/java'\n",
        "page = requests.get(main_url,headers={'user-agent':user_agent.chrome})\n",
        "soup = BeautifulSoup(page.content,'lxml')\n",
        "\n",
        "base_url = 'http://codingbat.com'\n",
        "\n",
        "all_divs = soup.find_all('div',class_='summ')\n",
        "\n",
        "\n",
        "# all_links has link for each section (Page 1)\n",
        "all_links = [base_url + div.a['href'] for div in all_divs] # This is list Comprahension\n",
        "\n",
        "#--------End - Same as above Script ----------------------------\n",
        "\n",
        "#Below code is to get link for each/all the section\n",
        "\n",
        "for link in all_links:\n",
        "    #link correspons to 2nd page ex:https://codingbat.com/java/Warmup-1\n",
        "    inner_page = requests.get(link,headers={'user-agent':user_agent.chrome})\n",
        "    inner_soup = BeautifulSoup(inner_page.content,'lxml')\n",
        "\n",
        "    #Now we need to scrap the link from 2nd inner page. (Inspect the HTML Page)\n",
        "\n",
        "    div = inner_soup.find('div',class_='tabc')\n",
        "    question_links = [base_url + td.a['href'] for td in div.table.find_all('td')] # has link to all the questions (list Comprahension)\n",
        "    print(question_links)\n",
        "\n",
        "    break #on commenting it you will get complete links for all the sections\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['http://codingbat.com/prob/p187868', 'http://codingbat.com/prob/p181646', 'http://codingbat.com/prob/p154485', 'http://codingbat.com/prob/p116624', 'http://codingbat.com/prob/p140449', 'http://codingbat.com/prob/p182873', 'http://codingbat.com/prob/p184004', 'http://codingbat.com/prob/p159227', 'http://codingbat.com/prob/p191914', 'http://codingbat.com/prob/p190570', 'http://codingbat.com/prob/p123384', 'http://codingbat.com/prob/p136351', 'http://codingbat.com/prob/p161642', 'http://codingbat.com/prob/p112564', 'http://codingbat.com/prob/p183592', 'http://codingbat.com/prob/p191022', 'http://codingbat.com/prob/p192082', 'http://codingbat.com/prob/p144535', 'http://codingbat.com/prob/p178986', 'http://codingbat.com/prob/p165701', 'http://codingbat.com/prob/p100905', 'http://codingbat.com/prob/p151713', 'http://codingbat.com/prob/p199720', 'http://codingbat.com/prob/p101887', 'http://codingbat.com/prob/p172021', 'http://codingbat.com/prob/p132134', 'http://codingbat.com/prob/p177372', 'http://codingbat.com/prob/p173784', 'http://codingbat.com/prob/p125339', 'http://codingbat.com/prob/p125268', 'http://codingbat.com/prob/p196441']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RjzeOxMvW1m"
      },
      "source": [
        "#Final Script\n",
        "\n",
        "#part 1\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "\n",
        "user_agent = UserAgent()\n",
        "main_url = 'http://codingbat.com/java'\n",
        "page = requests.get(main_url,headers={'user-agent':user_agent.chrome})\n",
        "soup = BeautifulSoup(page.content,'lxml')\n",
        "\n",
        "base_url = 'http://codingbat.com'\n",
        "\n",
        "all_divs = soup.find_all('div',class_='summ')\n",
        "\n",
        "all_links = [base_url + div.a['href'] for div in all_divs]\n",
        "\n",
        "\n",
        "# part 2\n",
        "\n",
        "for link in all_links:\n",
        "    inner_page = requests.get(link,headers={'user-agent':user_agent.chrome})\n",
        "    inner_soup = BeautifulSoup(inner_page.content,'lxml')\n",
        "    div = inner_soup.find('div',class_='tabc')\n",
        "    question_links = [base_url + td.a['href'] for td in div.table.find_all('td')]\n",
        "\n",
        "\n",
        "# part 3\n",
        "\n",
        "    for question_link in question_links:\n",
        "        final_page = requests.get(question_link)\n",
        "        final_soup = BeautifulSoup(final_page.content, 'lxml')\n",
        "        indent_div = final_soup.find('div', attrs={'class':'indent'})\n",
        "\n",
        "        problem_statement = indent_div.table.div.string\n",
        "\n",
        "        siblings_of_statement = indent_div.table.div.next_siblings\n",
        "\n",
        "        examples = [sibling for sibling in siblings_of_statement if sibling.string is not None]\n",
        "\n",
        "        print(problem_statement)\n",
        "        for example in examples:\n",
        "            print(example)\n",
        "\n",
        "        print('\\n\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}